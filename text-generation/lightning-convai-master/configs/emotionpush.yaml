# ------------------------  PyTorch Lightning Configurations --------------------------------------
seed: 12                                  # Training seed set everywhere
verbose: False                            # Verbosity level
experiment_name: gpt2-medium # experiment name

# ----------------------------- Early Stopping ----------------------------------------------------
monitor: val_nll                          # Metric to monitor during training
min_delta: 0.0                            # Sensitivity to the metric.
patience: 12                              # Number of epochs without improvement before stopping training    
metric_mode: min                          # 'min' or 'max' depending if we wish to maximize or minimize the metric

# ----------------------------- Model Checkpoint --------------------------------------------------
save_top_k: 1                             # How many checkpoints we want to save.
save_weights_only: True                   # Saves the model weights only

# ----------------------------- Lightning Trainer --------------------------------------------------
gradient_clip_val: 1.0                    # Clips gradients when the norm value exceeds 1.0
gpus: 1                                   # Number of GPUs to use. (1 is recommended)
deterministic: True                       # if true enables cudnn.deterministic. Might make your system slower, but ensures reproducibility.
overfit_batches: 0.0                      # DEBUG: Uses this much data of the training set. If nonzero, will use the same training set for validation and testing.
accumulate_grad_batches: 8                # Gradient accumulation steps
min_epochs: 1                             # Min number of epochs
max_epochs: 40                            # Max number of epochs
# limit_train_batches: 0.4                # To train with a lower percentage of the training data you can use this flag
# limit_val_batches: 500                  # Same as the previous flag but for validation. 
val_check_interval: 0.25                  # How often within one training epoch to check the validation set. Can specify as float or int.
#precision: 16                           # Train with 16 bit precision
# profiler: True                          # To profile individual steps during training and assist in identifying bottlenecks.
# resume_from_checkpoint: checkpoint.ckpt # To resume training from a specific checkpoint pass in the path here.
log_every_n_steps: 500

# --------------------------------- Dataset -------------------------------------------------------
pretrained_model: gpt2-medium
dataset_path: ../../data/emotionpush/     # By default we used the PersonaChat corpus but if you respect the same structure other data can be used.
dataset: emotionpush                      # Options: emotionlines, emotionpush, dailydialog, scenariosa
batch_size: 2                             # Batch size used during training.
max_history: 2                            # Max number of context sentences
personality_permutations: 1               # Max number of personality sentences
num_candidates: 4                         # Number of distractors 

# -------------------------------- GPT2 Fine-tuning -----------------------------------------------
learning_rate: 5.0e-6                    # Learning rate to be used during fine-tuning
lm_coef: 1.0                              # Language model loss weight
mc_coef: 1.0                              # Multiple-choice loss weight
    
# -------------------------------- Options --------------------------------------------------------
sentiment_representation: None                # type of representation to use: tag, words-set, random-sample, random-sentiment-sentence, context-sentence
words_set_path: ../../data/emotionpush/TF_IDF_1_3.json # path to the words set file 